{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Cooking  in Python\n",
    "https://www.kaggle.com/manuelatadvice/whats-cooking/noname/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#This imports a bunch of packages.  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#If you import the codes locally, this seems to cause some issues.  \n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "urltrain= 'https://raw.githubusercontent.com/RPI-Analytics/MGMT6963-2015/master/data/whatscooking/whatscookingtrain.json'\n",
    "urltest = 'https://raw.githubusercontent.com/RPI-Analytics/MGMT6963-2015/master/data/whatscooking/whatscookingtest.json'\n",
    "\n",
    "\n",
    "train = pd.read_json(urlopen(urltrain))\n",
    "test = pd.read_json(urlopen(urltest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cuisine\n",
       "brazilian        467\n",
       "british          804\n",
       "cajun_creole    1546\n",
       "chinese         2673\n",
       "filipino         755\n",
       "french          2646\n",
       "greek           1175\n",
       "indian          3003\n",
       "irish            667\n",
       "italian         7838\n",
       "jamaican         526\n",
       "japanese        1423\n",
       "korean           830\n",
       "mexican         6438\n",
       "moroccan         821\n",
       "russian          489\n",
       "southern_us     4320\n",
       "spanish          989\n",
       "thai            1539\n",
       "vietnamese       825\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First we want to see the most popular cuisine for the naive model. \n",
    "train.groupby('cuisine').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we write the most popular selction.  This is the baseline by which we will judge other models. \n",
    "test['cuisine']='italian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#THis is a much more simple version that selects out the columns ID and cuisinte\n",
    "submission=test[['id' ,  'cuisine' ]]\n",
    "#This is a more complex method I showed that gives same.\n",
    "#submission=pd.DataFrame(test.ix[:,['id' ,  'cuisine' ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This outputs the file.\n",
    "submission.to_csv(\"1_cookingSubmission.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So it seems there is some data we need to use the NLTK leemmatizer.  \n",
    "stemmer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We see this in a Python Solution. \n",
    "train['ingredients_clean_string1'] = [' , '.join(z).strip() for z in train['ingredients']] \n",
    "\n",
    "#We also know that we can do something similar though a Lambda function. \n",
    "strip = lambda x: ' , '.join(x).strip() \n",
    "#Finally, we call the function for name\n",
    "train['ingredients_clean_string2'] = train['ingredients'].map(strip)\n",
    "\n",
    "#Now that we used the lambda function, we can reuse this for the test dataset. \n",
    "test['ingredients_clean_string1'] = test['ingredients'].map(strip)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We see this in one of the solutions.  We can reconstruct it in a way that makes it abit easier to follow, but I found when doing that it took forever.  \n",
    "\n",
    "#To interpret this, read from right to left. \n",
    "train['ingredients_string1'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in train['ingredients']]       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we will create a corpus.\n",
    "corpustr = train['ingredients_string1']\n",
    "corpusts = test['ingredients_string1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "#You could develop an understanding based on each.  \n",
    "vectorizertr = TfidfVectorizer(stop_words='english',\n",
    "                             ngram_range = ( 1 , 1 ),analyzer=\"word\", \n",
    "                             max_df = .57 , binary=False , token_pattern=r'\\w+' , sublinear_tf=False)\n",
    "vectorizerts = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Note that this doesn't work with the #todense option.  \n",
    "tfidftr=vectorizertr.fit_transform(corpustr)\n",
    "predictors_tr = tfidftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9944x2963 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 182346 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note that this doesn't work with the #todense option.  This creates a matrix of predictors from the corpus. \n",
    "tfidfts=vectorizertr.transform(corpusts)\n",
    "predictors_ts= tfidfts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is target variable.  \n",
    "targets_tr = train['cuisine']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression. \n",
    "parameters = {'C':[1, 10]}\n",
    "#clf = LinearSVC()\n",
    "clf = LogisticRegression()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This uses that associated paramters to search a grid space. \n",
    "classifier = grid_search.GridSearchCV(clf, parameters)\n",
    "classifier=classifier.fit(predictors_tr,targets_tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This predicts the outcome for the test set. \n",
    "predictions=classifier.predict(predictors_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This adds it to the resulting dataframe. \n",
    "test['cuisine'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This creates the submision dataframe\n",
    "submission2=test[['id' ,  'cuisine' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This outputs the file.\n",
    "submission2.to_csv(\"2_logisticSubmission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c2e11fff634a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create the random forest object which will include all the parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# for the fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mforest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the random forest object which will include all the parameters\n",
    "# for the fit\n",
    "forest = RandomForestClassifier(n_estimators = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the training data to the Survived labels and create the decision trees\n",
    "forest = forest.fit(predictors_tr,targets_tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the same decision trees and run it on the test data\n",
    "predictions = forest.predict(predictors_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This adds it to the resulting dataframe. \n",
    "test['cuisine'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This creates the submision dataframe\n",
    "submission2=test[['id' ,  'cuisine' ]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
