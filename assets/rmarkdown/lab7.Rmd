---
title: "lab7"
output: html_document
---

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
summary(cars)
```

You can also embed plots, for example:

```{r, echo=FALSE}
plot(cars)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}



#The Iris Data 
#http://cran.r-project.org/doc/contrib/Zhao_R_and_data_mining.pdf 
#See Section 1.3
#The data is included with R, so we just have to call it. 
#This is a traditional "Classification" Problem

iris<-iris
summary (iris)
str(iris)
hist(iris$Sepal.Length)
plot(density(iris$Sepal.Length))
pie(table(iris$Species))
cor(iris[,1:4])
aggregate(Sepal.Length ~ Species, summary, data=iris)
boxplot(Sepal.Length~Species, data=iris)
install.packages("scatterplot3d")
library(scatterplot3d)
scatterplot3d(iris$Petal.Width, iris$Sepal.Length, iris$Sepal.Width)
install.packages("scatterplot3d")
library(MASS)
parcoord(iris[1:4], col=iris$Species)
install.packages("ggplot2")
library(ggplot2)
qplot(Sepal.Length, Sepal.Width, data=iris, facets=Species ~.)

#1. Desribe what you find through visualizing the data. What intuition 
#would a classifier have to capture to categorize the data?

#This pulls out 70% for training data.
#This starts on P. 29 of http://cran.r-project.org/doc/contrib/Zhao_R_and_data_mining.pdf 

#This creates a vector indicator we will use to select our 
#training and test dataset 
set.seed(1234)
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))

#Select our training data
trainData <- iris[ind==1,]

#Select our test data
testData <- iris[ind==2,]
#This is a package that includes a decision Tree 
install.packages('party')
library(party)
#This specifies the forula
myFormula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
#This generates the decision tree
iris_ctree <- ctree(myFormula, data=trainData)


#This creats a new variable 
trainData$pred_ctree<-predict(iris_ctree)
plot(iris_ctree)

table(trainData$pred_ctree, trainData$Species)
trainData$correct<-ifelse(trainData$pred_ctree==trainData$Species, 1, 0)

#This shows the percentage of the data correctly predicted.
summary(trainData$correct)

#2 Which class was most likely to be misclassified?  How many total were misclassified?
#3 Explain the logic found in the first node of the decision tree.    
#4 Rerun the analysis using only 20% of the data as a training.  What % is correct?

plot(iris_ctree, type="simple")
testData$pred_ctree <- predict(iris_ctree, newdata = testData)
testData$correct<-ifelse(testData$pred_ctree==testData$Species, 1, 0)

#5 What % of the predictions are correct in the test data?
#6 Would you expect the % classified correctly to be higher
#in the training or test analysis?


install.packages('randomForest')
library('randomForest')
rf <- randomForest(Species ~ ., data=trainData, ntree=100, proximity=TRUE)
trainData$pred_rf<-predict(rf)
table(trainData$pred_rf, trainData$Species)
print(rf) 
plot(rf)
importance(rf)


#7 Calculate the % classified corectly in the random forest prediction 
# for both the training and the test set.  

#8 What does "importance" tell us?

#K-means is an example of Unsupervised training.

iris2 <- iris[,1:4]
kmeans<- kmeans(iris2, 3)
iris$cluster<-kmeans$cluster
plot(iris2[c("Sepal.Length", "Sepal.Width")], col = kmeans$cluster)
points(kmeans$centers[,c("Sepal.Length", "Sepal.Width")], col=1:3, pch=8, cex=2)
table(iris$cluster, iris$Species)

#9  Which of the species is most separated between clusters?

install.packages('fpc')
library('fpc')
pamk <- pamk(iris2, 3)
pam <- pam(iris2, 3)

#10 the commands PAM and PAMK are two alternate clustering algorithms. 
#Assess relative ability to separate the data into clusters that 
#correspond with the data. 
install.packages('mlbench')
library(mlbench)
#http://rss.acs.unt.edu/Rdoc/library/mlbench/html/BostonHousing.html 
data(BostonHousing)
#medv   median value of owner-occupied homes in USD 1000's
# inspect the range which is 1-50
summary(BostonHousing$medv)


##
## model linear regression
##

lm.fit <- lm(medv ~ ., data=BostonHousing)

lm.predict <- predict(lm.fit)

summary(lm.fit)
#mean=22.53
mean(BostonHousing$medv)
# mean squared error: 21.89483
mean((lm.predict - BostonHousing$medv)^2) 

#This manually calcuates the R2
1-sum((lm.predict - BostonHousing$medv)^2)/sum((BostonHousing$medv-mean(BostonHousing$medv))^2)


plot(BostonHousing$medv, lm.predict,
     main="Linear regression predictions vs actual",
     xlab="Actual")


##
## model neural network
##
install.packages('nnet')
require(nnet)

# scale inputs: divide by 50 to get 0-1 range
nnet.fit <- nnet(medv/50 ~ ., data=BostonHousing, size=2) 
summary(nnet.fit)
# multiply 50 to restore original scale
nnet.predict <- predict(nnet.fit)*50 

# mean squared error: 16.40581
mean((nnet.predict - BostonHousing$medv)^2) 


plot(BostonHousing$medv, nnet.predict,
     main="Neural network predictions vs actual",
     xlab="Actual")

#11  Calculate the R2 that results from the nnet prediction.  

#12 Is the nnet prediction better or worse than the regression result?
#Why?


```
